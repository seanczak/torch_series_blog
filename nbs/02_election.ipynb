{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What in the World Happened in Virginia Tuesday Night?!\n",
    "> Statistically speaking, Your Vote Matters\n",
    "\n",
    "TL;DR\n",
    "- Stats is magic\n",
    "- Unbiased sampling is hard\n",
    "\n",
    "Like most American families, Tuesday night my mom and I closed up work around 5pm, heated up some leftovers, and  hundled around a TV to commence what would turn into a long night of bingeing/\"playing along at home\" with the election results.  Most of our analytics fix was generously provided for by CNN (oop, just called ourselves out with that one).  But we're not your ordinary group of sheep-le over here. No no, we're some of the more savvy consumers of click-bait information. We also used the Google!! And what we (and many others probably) noticed early on, was that there was a discrepency between Google's little macro (powered by AP) and CNN's classic US map dashboard (although it definitely is getting fancier). The numbers being reported were the same but a difference in information delivery paradigm caused an interesting bit of confusion in our household.  Allow me to explain... \n",
    "\n",
    "If a state had more Republicans votes at the time, CNN would shade it red (or blue for the opposing case) which gave the impression that the candidate had won or at least was \"winning\" the state.  Fine, makes sense.  However, Google took it one step further. They provided two \"grades\" of shading!  A light color meant the candidate was leading the popular vote of that state whereas a darker won meant that the candidate had \"won\" the state.  Tip of the hat for the addition of this clever, intuitive visualization feature.\n",
    "\n",
    "<img src=\"election/election_images/google.png\" width=600 height=600 />\n",
    "\n",
    "So what's the problem? Shouldn't the candidate that is winning the state's popular vote be the one whom the analytics-gods inevitably deem as the \"winner\" of that state? Not necessarily.  Follow me on this journey as we try to generate high-level, accesible, statistics-based logic in an attempt to answer\n",
    "- Why/how you can call an election with less than 10% of the vote\n",
    "- How a candidate can be losing by 20% and still be deemed the victor with ~2/3 of the votes remaining (Biden in Virginia at ~8:30pm EST)\n",
    "- Why couldn't we apply the logic for the above to Pennsylvania, Nevada, etc?\n",
    "- Does my vote actually count?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Big Deal about Calling Elections Early\n",
    "\n",
    "Let's take a look at Illinois for example. At 8:40pm EST, they had counted about 500 thousand votes which they estimated was about 6% of the votes.  How can a candidate win so early? Is this some sort of fake news/voter suppression?\n",
    "\n",
    "What I imagine happened here is a simple risk analysis coupled with the all-American drive to be FIRST.  On the one hand, it seems reasonable that at a certain point, we pretty much \"know\" who won right?  On the other, you don't want to be wrong about something like that.  These news sources are constantly competing for our eyeballs (and trust). Being first and not being wrong becomes a balancing act.  Imagine how many hits the first 10 sentence article got when Michigan was called (translation: `$$$$`).  Better call in those weirdos who enjoy doing math. See if they can sort this out..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK... So... How?\n",
    "I am not privy to the information that they are using to make these calls but I can show you something that I find kind of cool which at least explains what I believe is going on behind the scenes (if not a significantly more complex version). \n",
    "\n",
    "I start, like any self-respecting statistician, by assuming that the votes are not actually votes but marbles.  But not just ANY marbles. These are special voting marbles whose colors represent a person's vote (let's get a little crazy and say blue for Biden and red for Trump).  And instead of going to various polling offices, mail boxes, etc, the people cast their votes by bringing their special marbles to one big-ol marble bag located in the state's capital on election day.  Now, once that's been done, the state has a huge bag of marbles which effectively represents who the people of that state want to have as their next steward of the human-race-self-destruct button.\n",
    "\n",
    "<img src=\"election/election_images/SingleBag.png\" width=600 height=600 />\n",
    "\n",
    "Ok, now the fun begins.  We first shake the bag to make sure it's fully mixed.  Then we start drawing marbles and recording the results.  We get a better sense of what the composition of red/blue is of the bag as we draw each new marble. We know that if the composition of the bag is more than 50% red, then Trump wins.  Otherwise, Biden will win.  So let's phrase the problem this way, how many of these marbles need to be drawn in order to be confident in assigning a victor (e.g. saying that the bag has more or less than 50% of a single color). But how does one know when to stop (or at least allow the winners to announce \"hooray for our side\")?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing God\n",
    "\n",
    "Ok let's quantify that somehow.  Let's say there are a million people in Illinois and 55% of them voted for Biden.  This means the composition of that magic bag is 55% blue and 45% red. But no one knows it yet!  But for a moment, let's pretend like we do know it... maybe we're God. So we watch the humans start by drawing 100 marbles and they see that only 49 of them are blue.  The humans, being rational beings, conclude that as far as they know, the bag's composition is 49% blue and thus Trump wins.  As God, we might be confused here... how did they get it wrong? What were the odds of the humans drawing less than 50% blues?  To answer we decide run the same experiment (draw 100 from the bag and then put them back) a million times and plot the number of each observed outcome (shown below).  Ahh, now we see, in 30% of the cases, the humans could have drawn less than 50 blue marbles (see the red line) and incorrectly concluded that the bag had more red marbles. Ok it all checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/god1.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the humans are smart. They also realize that they have a high probability of getting the answer wrong if they only grab 100 marbles. So they quickly increase their number to 1000 marbles.  Before telling you what they saw, lets first try to make a guess.  A visual way to approach this would be to make the same graph as above (draw n = 1000 marbles from the bag a million times and plot the results) and see what that tells us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src=\"election/election_images/god2.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see in the overwhelming majority of the cases are above the necessary 501 blue marbles (see red) for the humans to get the right answer. In fact, these silly humans have less than 0.1% chance of the answer wrong now (or a 99.9% chance of getting it right).  Ok we feel better now. Let the humans keep counting.  We sit back with the rest of the world and turn on CNN/FOX news and let the drama unfold.\n",
    "\n",
    "But wait, we're not God (at least I'm not... I do hold my readers in high esteem though).  So while God may feel comfortable with the above exercise, we can't draw 1000 marbles a million times just to see how accurate our 1000 marble sample is.  That would mean we'd have drawn a billion marbles just to say something about how confident we can be with our 1000 marbles!  At that rate, we should just give up and count them all... right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference to the Rescue\n",
    "\n",
    "As noted above, the question we'd like to answer is: what is the composition of the marbles in the bag?  In other words, what percentage/proportion of the marbles are blue and what percentage/proportion are red? Since Biden wins Illinois, let's call the porportion of blue marbles \"$p$\" which allows us to say that if $p$>50% (also written as $p$ > 0.5), then Biden wins.  We've already established that we're going to use a sample of $n$ marbles to estimate this $p$. Our guess will be whatever the proportion of blue marbles we observe in our drawing (naturally).  Since this guess (called an estimator) is not necessarily the true value $p$ inside the bag, let's call it something else that's similar but not the same. In stats we give $p$ a cute little hat like this, $\\hat{p}$ to say: \"this is not actually $p$ but it's our best guess based on some data.\" Ok so to recap\n",
    "\n",
    "> make a figure for this\n",
    "\n",
    "- $p$: actual proportion of marbles that are blue\n",
    "- $\\hat{p}$: our observed proportion of blue marbles in a sample drawn from the bag (serves as our best guess at p)\n",
    "- $n$: the number of marbles drawn from the bag\n",
    "\n",
    "And we can rephrase our problem in a simplified manner to start to tackle in systematically.\n",
    "\n",
    "> **Goals:**\n",
    "- Determine values our marble bag's $p$ could be. (Note: if $p$ > 50% blue then Biden wins, else Trump wins)\n",
    "\n",
    "> **Constraints:**\n",
    "- Do it with as few counted marbles, $n$, as possible\n",
    "- Don't be wrong\n",
    "\n",
    "If only there was a way to say, I'm 99.99% confident that $p$ is between these two numbers.  I'm glad you asked because that is exactly what we invented confidence intervals for.  They give us a way to take some observed data and use it to infer something about the world that produced that data.  In our case, we want to know a range that we believe the marble bag's proportion of blue marbles $p$ will fall into (remember if we know $p$ we know who won). So how do we do this. Well, that's where statistical inference comes in. In order to be accessible, I plan to be brief about this and then we'll jump back. \n",
    "\n",
    "{deep inhaling}\n",
    "\n",
    "It turns out that the marble drawing situation is called a hypergeometric process.  For those of you who don't care what that is, just know that the important part is that it has a name which means that someone smart figured out what to do with it. In fact, that was how I simulated all the plots so far (python will just do it for you if you tell it the name of the distribution). Going deeper it turns out that when the number of marbles in the bag increases it approaches a different (easier to work with) situation called a binomial process. The only reason I bring any of this up is that the Binomial process has one important parameter and it's called, you guessed it $p$ which we can easily approximate a confidence interval for.  I don't plan on going into the approach that approximates the confidence interval on $p$ but I'll say that if you're interested look up some of these terms: binomial distribution, confidence interval, Bayesian inference, conjugate prior. (This feels like I cop-out so I might write another blog just about this, stay tuned.)\n",
    "\n",
    "{big exhale}\n",
    "\n",
    "For the rest of us, let's just look at pictures shall we? Let's say that we were those humans from before that had drawn 49/100 red balls from a bag that contains only God knows how many blue balls (again, $p$ = proportion blue balls).  So our $\\hat{p}$ estimated from this is... you guessed it 0.49 (that's how statisticians write 49%). Given that n=100, what can we say about $p$?  Well, using the aforementioned, not to be mentioned again, Bayesian inference approach, we can generate a guess as what values $p$ (of the bag) could be after selecting 49/100 blue marbles ($n$=100).  And of course, this is visualized as a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/beta_n100.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line is at the decision threshold where if $p$ is to the left - Trump wins and if it's to the right - Biden wins.  We see that there is a decent probability that $p$ could be anywhere from 0.35 to 0.65. Thus, due to the large \"spread\" in possible values for $p$, we still don't know who will win. Of course, the majority of the probability is centered around $p$ being 0.49 (which we know to be wrong due to an unlucky draw). So let's keep drawing and increase n = 1000.  Now the results probably reflect something more realistic like, say, 541 blue balls (i.e. $\\hat{p}$ ~ 0.541) which is closer to bag's true proportion of blue balls ($p$ = 0.55). The magenta lines I've drawn on the following plots show the boundaries which hold 99.99% of the probability between them. Since we have a 99.99% confidence that $p$ is between these two lines, any value outside those bounds represents an unimaginably, unlikely event.  When we do this, the spread of the distribution greatly decreases (i.e. the distribution has gotten thinner). This means that even though we haven't counted all the samples, we have a very narrow window with which to guess $p$ and none of them spell victory for Trump. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/beta_n1000.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we increased $n$ again, now to 10,000 marbles counted, we can see that the spread continues to shrink to almost nothing. In fact, it shrinks so much that we see that the probability that $p$ is less than 0.5 (Trump winning) is essentially non-existent.  So by the time we get to this point in the counting process, we are already 99.99% confident that $p$ is between 0.53 and 0.57.  In other words, even before counting 10,000 marbles, we knew that Biden would win.\n",
    "\n",
    "<img src=\"election/election_images/beta_n10000.png\" width=800 height=600 />\n",
    "\n",
    "But... that doesn't seem like that many votes? And... what about Virginia?  Keep reading my friends.  \n",
    "\n",
    "There were two assumptions that I made (and kind of hid) from you in the creation of this fake marble-selecting world which definitely do not hold in real life. These being:\n",
    "- The marbles were well mixed (so as to allow equal opportunity for any to be selected)\n",
    "- There is only one bag\n",
    "\n",
    "But the main takeaway remains: in the case of a well mixed bag of marbles, you don't need to select that many marbles in order to determine what the composition of the rest of the bag is.  I find that fascinating and hard to believe sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virginia's Story\n",
    "\n",
    "### Setting the scene\n",
    "Take Virginia for example.  At around 8:30pm EST, state officials had counted 33% of votes and Biden was losing by 20%.  However, the Google macro had shaded this state dark blue to signify an unquestionable Biden victory.  Ok, what the heck?  My mom felt that way and it wasn't helped by the television pundits who kept the game-show illusion going with the rhetoric that \"hopefully Biden pulls back in Virginia.\"  This confusion is what inspired this post in fact.  The statisticians new something that we as viewers were not being told.\n",
    "\n",
    "Ok let's go back to the bag of marbles example. We already said that this is unrealistic because there are more than one bag of marbles. In fact, there are 133 counties and over 2500 precincts in Virginia.  If we were to use the one bag model above, we'd make the eggregious error of assuming that we could draw from any of these bags and make a general statement about the entire state.  Well why not?  That's how the information is displayed to us?  33% of votes have been counted at 8:30pm people! That's well over our 10,000 marble threshold that we mathematically proved above... close the books!\n",
    "\n",
    "So in lies the real work of a professional statistician: choosing representative samples is hard. But let's say that we needed to say SOMETHING even though we knew that our sample was biased.  Well, what if instead of assuming a single bag of votes, we assumed that each county was a smaller bag within that bag. Yes, the state's marble bag is actually a bag of county marble bags (the rabbit hole continues!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/SplitBagByCounty.png\" width=700 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But does this explain what happened Tuesday? Well let's look at the data.  I took some numbers directly off the Virginia state's official election reporting website sometime on 11/5 (votes should be in at that time).  The trend said that Biden had been awarded 53% of the counted votes.  Recalling Tuesday night, CNN would've had us believing that this was some sort of heroic push but I think the statisticians would argue that it was inevitable. Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking the data\n",
    "\n",
    "Ok let's start by doing a quick thought experiment to see if we can recreate the situation we saw at 8:30pm on Tues when Biden was losing by 20% but declared the winner.  Let's pretend that 10,000 votes are counted from each county (i.e. ~1.5 million of Virginia's ~4.5 million total votes).  The following is a plot of 30 randomly selected counties in Virginia and how they might've looked after counting their first 10,000 votes. (I've put a box around Fairfax County because it becomes important later - stay tuned.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/Virginia10kvotes.png\" width=1000 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I add up the votes from all 133 counties, we can see that Trump would have a commanding lead over Biden at this early stage (shown on the left below).  However, the statisticians look at the $\\hat{p}$'s of the individual counties above (i.e. what proportion voted of each voted for Biden) and realize that Trump has already lost.  What they might've done is they take the $\\hat{p}$ for a given county and used it to infer a range of values that $p$ might take in that county (i.e. what the county's bag contains) just like we did for Illinois above.  Then, they could take the high and low values of that interval and multiply them by the number of people that they expect to vote in that county. Then just repeat the process for each county and add up the totals for the low and high estimate cases (right figure). And with that, you can see that even the unimaginably worst case scenario for Biden (labeled \"low\" below), he still is well above the red \"line to win\" drawn at 50% of the votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/Virginia10k_totals.png\" width=1200 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, for those of you who followed all the way up until now you might still be asking: how could this be? Well, let's just look at the raw results from the 30 most populous counties and I feel like the answer should be obvious.  Check out how many votes Biden picked up in Fairfax County. In fact, he won the top 12 most populous counties in Virginia (in some cases by a landslide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"election/election_images/virginia_county_votes.png\" width=1000 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, votes aren't actually counted at the same speed and my little model I presented here isn't actually the numbers from Tuesday night (8:30pm). But... it shows that while the public is being fed one story about Trump leading by 20%, the statisticians have already realized he doesn't have a prayer of winning Virginia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swing States\n",
    "\n",
    "So we saved our simple marble bag model above by dividing it into a marble bag full of smaller marble bags.  But what we're seeing in this election is that not only does the county matter but how you chose to vote.  In a world where one candidate might tell his supporters to avoid voting by mail, you could expect to see a huge difference in the compositions of the \"voted by mail\" bag and the \"voted on election day\" bag.  What's more is that different ethnic groups may play a key role in some counties to swing a state based on how many vote that year.  You could keep going with this: gender, age groups, first-time voters, etc. Selecting a larger proportion of any of these voters could unnaturally bias your sample and give you an unrealistic expectation of $p$ (and thus the entire election). These are what we call \"confounding variables\" which I find to be a hilariously appropriate name for the things that confuse and keep statisticians up at night.  The role they play is to upset our assumption that the marble bag was \"well-mixed\" before we started sampling.  Without accounting for them, one really doesn't have a clue what is going on.\n",
    "\n",
    "So what does this mean? Well, first off, it means that there definitely is not a bunch of people waiting for a certain number of votes to come in before they can call the state (at the time of this writing Georgia still hadn't been called despite being at 99%). Instead, it means that there is a team of people who try to figure out what important trends to look out for in each state, each county, each demographic.  For example, one exhaustive way to account for confounding variables is to go back to the bags within bags approach we used for the counties.  Think about it like a set of Russian dolls. Within a state, you choose to separate different counties because you imagine people vote differently based on where they live. Then within the counties you assume that the way someone votes makes a difference in who they vote for.  And then you can keep going with ethnicities within each of those sub-groups and on and on.  Once you have a sense for how each of these tiny, tiny bags react, you can then make an prediction on the whole state's final tally.\n",
    "\n",
    "<img src=\"election/election_images/confounding.png\" width=1000 height=600 />\n",
    "\n",
    "Allow me to reiterate that I'm not privy to their process. I'm just a guy that works with statistics a bunch. The work we did above for Illinois and Virginia assumed that we had representative samples and that we knew the total number of voters (in each county and the state).  In that way, we were able to approximate the worst and best case scenarios for each candidate and make a guess based on those cases.  Unfortunately, none of those assumptions are valid but typically the difference in results between candidates is wide enough that such uncertainty is absorbed (i.e. one candidate wins by ~5% in that state).\n",
    "\n",
    "Swing states are a different story...\n",
    "\n",
    "### All Models are wrong, some are useful\n",
    "\n",
    "A swing state designates a nearly split population which results in such tight races that can sometimes be decided by less than a percent. I imagine its harder to estimate who will show up to vote and \n",
    "\n",
    "\n",
    "which way they will vote which adds more uncertainty.  Moreover, in the case of this election, we sometimes encounter situations which there is no precedent and, thus, extremely hard to predict.  The massive numbers of mail-in ballots this year is something we've never experienced and we're seeing that it's really hard to predict exactly how many will come in. This was made clear when Pennsylvania counted 100k more votes and decreased their reporting precentage from 89% to 88% (it went backwards!). What's more is that some states allow for ballots to count even if they come in weeks late as long as they have been post-marked by a certain date.  With this situation, it's hard to know how big the bag of \"mail in ballots\" is even if you can already infer that ~70% of them are going to be in favor a single candidate.\n",
    "\n",
    "With all of this uncertainty wrapped up in the fundamental make up of the model, it makes it difficult to confidently declare a state.  The inherent flaws of the model that worked so well in Virginia is shown for what it really is in Georgia.  The statistician... has no clothes.  Compound that with the external pressure to make the right call (as exemplified by the inquisition put to this Fox News analyst https://www.youtube.com/watch?v=OJDKS3d2RHk&feature=emb_logo) and you get the situation that we're currently experiencing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Vote?\n",
    "\n",
    "You might be asking yourself: \"this is all interesting but if anything it's convinced me EVEN MORE that my vote doesn't matter.\" I'd argue that, statistically speaking, that's not true.  Let's go back to Virginia.\n",
    "\n",
    "So far in this article, we've shaped all our models around the idea that the big bag of marbles at the state capital was the \"population\" of votes that we were trying to estimate through sampling.  But, in fact, that big bag IS the sample! The state's total voting population is... well... the \"population of interest.\"  Sure if one person doesn't show up to vote, it might only slightly change the proportion of votes. But, technically speaking, it still could bias our sample if it happens in a way that is unbalanced. \n",
    "\n",
    "One way this could happen is through group-thinking. Having a cavalier attitude could lead to those around you having the same attitude and, in turn, propogating it further to their friends.  If people can be assumed to vote similarly to their friends, then a huge group of your network not voting could lead to an unbiased sample on election day.  Imagine if something like that had happened with Democrats in Fairfax County, Virginia?  It could've unnaturlly changed a predictable, landslide victory for Biden into a close-count as a result of biased sampling.  \n",
    "\n",
    "You make the call... As for me, I want my big bag of marbles at my state capital to be an accurate representation of my state's population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
